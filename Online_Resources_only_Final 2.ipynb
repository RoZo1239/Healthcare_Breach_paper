{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff6376-5fd3-428b-bcea-c19d2623f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, ttk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from ollama import chat\n",
    "import time\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "import random\n",
    "\n",
    "# Globals\n",
    "vectorizer = None\n",
    "classifier = None\n",
    "output_df = []  # List of dicts to convert into DataFrame\n",
    "\n",
    "# Train classifier\n",
    "def load_and_train_classifier(df):\n",
    "    global vectorizer, classifier\n",
    "    df = df.dropna(subset=[\"Web Description\", \"Type of Breach\"])\n",
    "    X = df[\"Web Description\"].astype(str)\n",
    "    y = df[\"Type of Breach\"].astype(str)\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_vec, y)\n",
    "\n",
    "# Predict breach type\n",
    "def predict_breach_type(desc):\n",
    "    return classifier.predict(vectorizer.transform([desc]))[0]\n",
    "\n",
    "# Ollama LLM\n",
    "def generate_ollama_response(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    try:\n",
    "        resp = chat(model=\"llama3\", messages=messages)\n",
    "        return resp.message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Ollama Error: {e}\"\n",
    "\n",
    "# Extract cause of breach from LLM response (simple heuristic)\n",
    "def extract_cause_of_breach(llm_response):\n",
    "    lines = llm_response.splitlines()\n",
    "    for line in lines:\n",
    "        low = line.lower()\n",
    "        if \"cause\" in low or \"likely cause\" in low:\n",
    "            return line.strip()\n",
    "    return lines[0].strip() if lines else \"\"\n",
    "\n",
    "# Update UI safely from thread\n",
    "def update_ui(i, entity, desc, breach, response):\n",
    "    result_text.insert(tk.END, f\"\\n--- Record {i+1} ---\\n\")\n",
    "    result_text.insert(tk.END, f\"Covered Entity: {entity}\\n\")\n",
    "    result_text.insert(tk.END, f\"Web Description: {desc}\\n\")\n",
    "    result_text.insert(tk.END, f\"Predicted Breach Type: {breach}\\n\")\n",
    "    result_text.insert(tk.END, f\"LLM Insight (Mitigation / Prevention / Online Resources):\\n{response}\\n\")\n",
    "    result_text.insert(tk.END, \"-\" * 80 + \"\\n\")\n",
    "    result_text.see(tk.END)\n",
    "\n",
    "# File processing\n",
    "def process_file(path):\n",
    "    global output_df\n",
    "    output_df = []  # Clear old results\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "    except Exception as e:\n",
    "        root.after(0, lambda: result_text.insert(tk.END, f\"Failed to read file: {e}\\n\"))\n",
    "        return\n",
    "\n",
    "    required_cols = {\"Web Description\", \"Type of Breach\", \"Name of Covered Entity\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        missing = required_cols - set(df.columns)\n",
    "        root.after(0, lambda: result_text.insert(tk.END, f\"Missing required columns: {missing}\\n\"))\n",
    "        return\n",
    "\n",
    "    load_and_train_classifier(df)\n",
    "\n",
    "    # --- Updated system prompt ---\n",
    "    sys_prompt = (\n",
    "        \"You are a cybersecurity research assistant. \"\n",
    "        \"For each healthcare breach description, write a structured analysis with three sections:\\n\"\n",
    "        \"1. Mitigation steps — practical actions an organization can take to reduce current impact.\\n\"\n",
    "        \"2. Prevention strategies — proactive measures to avoid similar incidents in the future.\\n\"\n",
    "        \"3. Online resources — list only freely available online cybersecurity or privacy resources \"\n",
    "        \"(such as security blogs, industry articles, open-source tools, awareness training sites, or reputable tech forums). \"\n",
    "        \"Avoid citing or referencing official frameworks, standards, or regulatory sites such as NIST, HIPAA, or HHS. \"\n",
    "        \"Make the resource list appear varied and random each time. \"\n",
    "        \"Do not summarize or quote guidelines — only refer to open online materials.\"\n",
    "    )\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row.get(\"Web Description\", \"\") or \"\").strip()\n",
    "        entity = str(row.get(\"Name of Covered Entity\", \"Unknown Entity\")).strip()\n",
    "\n",
    "        if not desc:\n",
    "            continue\n",
    "\n",
    "        breach = predict_breach_type(desc)\n",
    "\n",
    "        variation_prompt = f\"Include a few randomly chosen online sources. Random tag: {random.randint(1000, 9999)}\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "Covered Entity: {entity}\n",
    "Predicted Breach Type: {breach}\n",
    "\n",
    "Please provide:\n",
    "1. Mitigation steps\n",
    "2. Prevention strategies\n",
    "3. Online resources (exclude NIST, HIPAA, HHS, and government frameworks)\n",
    "\n",
    "{variation_prompt}\n",
    "\"\"\"\n",
    "\n",
    "        # Call the Ollama LLM to get the response\n",
    "        response = generate_ollama_response(sys_prompt, user_prompt)\n",
    "\n",
    "        # Remove random tag line if echoed by the model\n",
    "        response = \"\\n\".join(line for line in response.splitlines() if \"random tag\" not in line.lower())\n",
    "\n",
    "        cause = extract_cause_of_breach(response)\n",
    "\n",
    "        root.after(0, update_ui, i, entity, desc, breach, response)\n",
    "\n",
    "        output_df.append({\n",
    "            \"Record #\": i + 1,\n",
    "            \"Covered Entity\": entity,\n",
    "            \"Web Description\": desc,\n",
    "            \"Predicted Breach Type\": breach,\n",
    "            \"Cause of Breach\": cause,\n",
    "            \"LLM Insight\": response\n",
    "        })\n",
    "\n",
    "        time.sleep(0.2)  # To avoid hammering the LLM\n",
    "\n",
    "# Save output to Word document\n",
    "def save_output():\n",
    "    if not output_df:\n",
    "        result_text.insert(tk.END, \"\\nNo data to save. Process a file first.\\n\")\n",
    "        return\n",
    "\n",
    "    save_path = filedialog.asksaveasfilename(defaultextension=\".docx\",\n",
    "                                             filetypes=[(\"Word Documents\", \"*.docx\")],\n",
    "                                             title=\"Save Output As\")\n",
    "    if save_path:\n",
    "        try:\n",
    "            doc = Document()\n",
    "            doc.add_heading('Breach Insight Report', level=1)\n",
    "\n",
    "            for record in output_df:\n",
    "                doc.add_heading(f\"Record {record['Record #']}\", level=2)\n",
    "\n",
    "                doc.add_paragraph(f\"Covered Entity: {record['Covered Entity']}\", style='Normal')\n",
    "                doc.add_paragraph(f\"Web Description:\\n{record['Web Description']}\", style='Normal')\n",
    "                doc.add_paragraph(f\"Predicted Breach Type: {record['Predicted Breach Type']}\", style='Normal')\n",
    "                doc.add_paragraph(f\"Cause of Breach: {record['Cause of Breach']}\", style='Normal')\n",
    "\n",
    "                doc.add_paragraph(\"LLM Insight (Mitigation / Prevention / Online Resources):\", style='Normal')\n",
    "                insight = doc.add_paragraph(record['LLM Insight'], style='Normal')\n",
    "                insight.paragraph_format.space_after = Pt(12)\n",
    "\n",
    "                doc.add_paragraph(\"-\" * 80, style='Normal')\n",
    "\n",
    "            doc.save(save_path)\n",
    "            result_text.insert(tk.END, f\"\\nOutput saved to: {save_path}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            result_text.insert(tk.END, f\"\\nFailed to save file: {e}\\n\")\n",
    "\n",
    "# File browser trigger\n",
    "def browse():\n",
    "    path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx\")])\n",
    "    if path:\n",
    "        result_text.delete(\"1.0\", tk.END)\n",
    "        global output_df\n",
    "        output_df = []\n",
    "        threading.Thread(target=process_file, args=(path,), daemon=True).start()\n",
    "\n",
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Breach Insight (Online Resources Only)\")\n",
    "\n",
    "frame = ttk.Frame(root, padding=10)\n",
    "frame.grid(row=0, column=0, sticky=(tk.W, tk.E))\n",
    "\n",
    "ttk.Label(frame, text=\"Upload Excel File:\").grid(column=0, row=0, sticky=tk.W)\n",
    "ttk.Button(frame, text=\"Browse\", command=browse).grid(column=1, row=0, padx=5)\n",
    "ttk.Button(frame, text=\"Download Output\", command=save_output).grid(column=2, row=0, padx=5)\n",
    "\n",
    "text_frame = ttk.Frame(root)\n",
    "text_frame.grid(row=1, column=0, padx=10, pady=10, sticky=\"nsew\")\n",
    "\n",
    "result_text = tk.Text(text_frame, wrap=\"word\", width=100, height=40)\n",
    "scrollbar = ttk.Scrollbar(text_frame, orient=\"vertical\", command=result_text.yview)\n",
    "result_text.configure(yscrollcommand=scrollbar.set)\n",
    "result_text.grid(row=0, column=0, sticky=\"nsew\")\n",
    "scrollbar.grid(row=0, column=1, sticky=\"ns\")\n",
    "\n",
    "text_frame.columnconfigure(0, weight=1)\n",
    "text_frame.rowconfigure(0, weight=1)\n",
    "root.columnconfigure(0, weight=1)\n",
    "root.rowconfigure(1, weight=1)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11587e63-e3ed-480f-b7a3-69a1aaaa018d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
